<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3d gaussian splating, HTML,CSS,JavaScript,JQuery,java,linuxç­‰">
    <meta name="description" content="3D Gaussian Splatting for Real-Time Radiance Field Rendering

è‹±æ–‡
 Fig. 1. Our method achieves real-time rendering of rad">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3d gaussian splating | zws</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    
    <style>
        body{
            background-image: url(https://c-ssl.duitang.com/uploads/item/202003/29/20200329173551_wvqha.jpeg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/atom.xml" title="zws" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="https://pic1.zhimg.com/v2-0c51d349e015a46ff4a1deceba95f728_xll.jpg?source=32738c0c" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">zws</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>å…³äº</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>ç•™è¨€æ¿</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>å‹æƒ…é“¾æ¥</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="https://pic1.zhimg.com/v2-0c51d349e015a46ff4a1deceba95f728_xll.jpg?source=32738c0c" class="logo-img circle responsive-img">
        
        <div class="logo-name">zws</div>
        <div class="logo-desc">
            
            æœ¬ç½‘ç«™æ˜¯ä¸ªäººå…´è¶£çˆ±å¥½ï¼Œæ€»ç»“åˆ†äº«ç»éªŒï¼Œè®°å½•ç”Ÿæ´»ç‚¹æ»´çš„å¹³å°ï¼Œå¸Œæœ›åœ¨ä»¥åçš„å­¦ä¹ æ—…é€”ä¸­ï¼Œèµ°å‡ºè‡ªå·±çš„é£æ™¯ã€‚
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			å…³äº
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			ç•™è¨€æ¿
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			å‹æƒ…é“¾æ¥
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/%5Cimg%5C2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3d gaussian splating</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/3dgs/">
                                <span class="chip bg-color">3dgs</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" class="post-category">
                                ä¸‰ç»´é‡å»º
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-03-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-03-30
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- æ˜¯å¦åŠ è½½ä½¿ç”¨è‡ªå¸¦çš„ prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        
        <!-- ä»£ç å—æŠ˜è¡Œ -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering"><a href="#3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering" class="headerlink" title="3D Gaussian Splatting for Real-Time Radiance Field Rendering"></a>3D Gaussian Splatting for Real-Time Radiance Field Rendering</h1><p><img src="/2024/03/30/3dgs/fig1.png" alt="Fig 1"></p>
<details>
<summary>è‹±æ–‡</summary>
 Fig. 1. Our method achieves real-time rendering of radiance fields with quality that equals the previous method with the best quality [Barron et al. 2022],while only requiring optimization times competitive with the fastest previous methods [Fridovich-Keil and Yu et al. 2022; MÃ¼ller et al. 2022]. Key to this performance is a novel 3D Gaussian scene representation coupled with a real-time differentiable renderer, which offers significant speedup to both scene optimization and novel view synthesis. Note that for comparable training times to InstantNGP [MÃ¼ller et al. 2022], we achieve similar quality to theirs; while
 this is the maximum quality they reach, by training for 51min we achieve state-of-the-art quality, even slightly better than Mip-NeRF360 [Barron et al. 2022].
 </details>
 <details>
<summary>ä¸­æ–‡</summary>
å›¾1. æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è¾å°„åœºçš„å®æ—¶æ¸²æŸ“ï¼Œå…¶è´¨é‡ä¸ä¹‹å‰å…·æœ‰æœ€ä½³è´¨é‡çš„æ–¹æ³•ç›¸å½“[Barronç­‰äººï¼Œ2022å¹´]ï¼ŒåŒæ—¶åªéœ€è¦ä¸æœ€å¿«çš„å…ˆå‰æ–¹æ³•[Fridovich-Keilå’ŒYuç­‰äººï¼Œ2022å¹´ï¼›MÃ¼llerç­‰äººï¼Œ2022å¹´]ç«äº‰åŠ›ç›¸å½“çš„ä¼˜åŒ–æ—¶é—´ã€‚è¿™ç§æ€§èƒ½çš„å…³é”®æ˜¯ä¸€ç§æ–°é¢–çš„3Dé«˜æ–¯åœºæ™¯è¡¨ç¤ºï¼Œä¸å®æ—¶å¯å¾®æ¸²æŸ“å™¨ç›¸ç»“åˆï¼Œè¿™æ˜¾è‘—åŠ å¿«äº†åœºæ™¯ä¼˜åŒ–å’Œæ–°è§†è§’åˆæˆçš„é€Ÿåº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸InstantNGP[MÃ¼llerç­‰äººï¼Œ2022å¹´]ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨ç±»ä¼¼çš„è®­ç»ƒæ—¶é—´å†…å®ç°äº†ç±»ä¼¼çš„è´¨é‡ï¼›è™½ç„¶è¿™æ˜¯ä»–ä»¬è¾¾åˆ°çš„æœ€é«˜è´¨é‡ï¼Œä½†é€šè¿‡51åˆ†é’Ÿçš„è®­ç»ƒï¼Œæˆ‘ä»¬å®ç°äº†æœ€å…ˆè¿›çš„è´¨é‡ï¼Œç”šè‡³ç•¥ä¼˜äºMip-NeRF360[Barronç­‰äººï¼Œ2022å¹´]ã€‚optimizationï¼šæœ€ä¼˜åŒ–ï¼Œæœ€ä½³é€‰æ‹©ï¼›comprtitive:ç«äº‰çš„ï¼Œä¸€æ ·å¥½çš„ï¼Œcouple:å¤«å¦»ï¼Œç»“åˆï¼Œè¿æ¥ï¼›speedupï¼šåŠ é€Ÿï¼›
</details>

<h3 id="æ‘˜è¦"><a href="#æ‘˜è¦" class="headerlink" title="æ‘˜è¦"></a>æ‘˜è¦</h3><details>
<summary>è‹±æ–‡</summary>       
Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates.Weintroduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (â‰¥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration,we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows real time rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.
</details>

<details>
<summary>ä¸­æ–‡</summary>
è¾å°„åœºæ–¹æ³•æœ€è¿‘åœ¨å¤šå¼ ç…§ç‰‡æˆ–è§†é¢‘ä¸­æ•è·çš„åœºæ™¯çš„æ–°è§†ç‚¹åˆæˆæ–¹é¢å–å¾—äº†é©å‘½æ€§çš„è¿›å±•ã€‚ç„¶è€Œï¼Œè¦å®ç°é«˜è´¨é‡çš„è§†è§‰æ•ˆæœä»ç„¶éœ€è¦æ˜‚è´µçš„ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒå’Œæ¸²æŸ“ï¼Œè€Œæœ€è¿‘çš„å¿«é€Ÿæ–¹æ³•ä¸å¯é¿å…åœ°åœ¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´åšå‡ºæƒè¡¡ã€‚å¯¹äºæ— ç•Œå’Œå®Œæ•´çš„åœºæ™¯ï¼ˆè€Œä¸ä»…ä»…æ˜¯å­¤ç«‹çš„å¯¹è±¡ï¼‰ä»¥åŠ1080påˆ†è¾¨ç‡çš„æ¸²æŸ“ï¼Œç›®å‰æ²¡æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥å®ç°å®æ—¶çš„æ˜¾ç¤ºé€Ÿç‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªå…³é”®è¦ç´ ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¿æŒç«äº‰åŠ›çš„åŸ¹è®­æ—¶é—´çš„åŒæ—¶å®ç°æœ€å…ˆè¿›çš„è§†è§‰è´¨é‡ï¼Œå¹¶ä¸”é‡è¦çš„æ˜¯å…è®¸åœ¨1080påˆ†è¾¨ç‡ä¸‹è¿›è¡Œé«˜è´¨é‡çš„å®æ—¶ï¼ˆâ‰¥ 30 fpsï¼‰æ–°è§†ç‚¹åˆæˆã€‚é¦–å…ˆï¼Œä»ç›¸æœºæ ¡å‡†æœŸé—´äº§ç”Ÿçš„ç¨€ç–ç‚¹å¼€å§‹ï¼Œæˆ‘ä»¬ä½¿ç”¨3Dé«˜æ–¯æ¥è¡¨ç¤ºåœºæ™¯ï¼Œè¿™äº›é«˜æ–¯ä¿ç•™äº†è¿ç»­ä½“ç§¯è¾å°„åœºçš„ç†æƒ³ç‰¹æ€§ï¼Œç”¨äºåœºæ™¯ä¼˜åŒ–ï¼ŒåŒæ—¶é¿å…äº†åœ¨ç©ºç™½ç©ºé—´ä¸­è¿›è¡Œä¸å¿…è¦çš„è®¡ç®—ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯¹3Dé«˜æ–¯è¿›è¡Œäº¤æ›¿ä¼˜åŒ–/å¯†åº¦æ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯ä¼˜åŒ–å„å‘å¼‚æ€§åæ–¹å·®ï¼Œä»¥å®ç°å¯¹åœºæ™¯çš„å‡†ç¡®è¡¨ç¤ºï¼›ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¿«é€Ÿçš„å¯è§æ€§æ„ŸçŸ¥æ¸²æŸ“ç®—æ³•ï¼Œæ”¯æŒå„å‘å¼‚æ€§çš„å–·æ´’ï¼Œå¹¶ä¸”æ—¢åŠ é€Ÿäº†è®­ç»ƒï¼Œåˆå…è®¸å®æ—¶æ¸²æŸ“ã€‚æˆ‘ä»¬åœ¨å‡ ä¸ªå·²å»ºç«‹çš„æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„è§†è§‰è´¨é‡å’Œå®æ—¶æ¸²æŸ“ã€‚ revolutionizedï¼šæ”¹å˜äº†ï¼›
</details>


<h3 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h3>  <details>
<summary>è‹±æ–‡</summary> 
 Meshes and points are the most common 3D scene representations
 becausetheyareexplicitandareagoodfitforfastGPU/CUDA-based
 rasterization. In contrast, recent Neural Radiance Field (NeRF) meth
ods build on continuous scene representations, typically optimizing
 a Multi-Layer Perceptron (MLP) using volumetric ray-marching for
 novel-view synthesis of captured scenes. Similarly, the most efficient
 radiance field solutions to date build on continuous representations
 by interpolating values stored in, e.g., voxel [Fridovich-Keil and Yu
 et al. 2022] or hash [MÃ¼ller et al. 2022] grids or points [Xu et al. 2022].
 While the continuous nature of these methods helps optimization,
 the stochastic sampling required for rendering is costly and can
 result in noise. We introduce a new approach that combines the best
 of both worlds: our 3D Gaussian representation allows optimization
 with state-of-the-art (SOTA) visual quality and competitive training
 times, while our tile-based splatting solution ensures real-time ren
dering at SOTA quality for 1080p resolution on several previously
 published datasets [Barron et al. 2022; Hedman et al. 2018; Knapitsch
 et al. 2017] (see Fig. 1).
</details>
 <details>
<summary>ä¸­æ–‡</summary>
ç½‘æ ¼å’Œç‚¹äº‘æ˜¯æœ€å¸¸è§çš„3Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æ˜¾å¼çš„ä¸”é€‚ç”¨äºå¿«é€Ÿçš„GPU/CUDAæ¸²æŸ“ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç¥ç»è¾å°„åœºï¼ˆNeural Radiance Fieldï¼Œç®€ç§°NeRFï¼‰æ–¹æ³•é‡‡ç”¨äº†è¿ç»­çš„åœºæ™¯è¡¨ç¤ºï¼Œé€šå¸¸é€šè¿‡ä½“ç§¯å…‰çº¿æŠ•å°„æ¥ä¼˜åŒ–å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMulti-Layer Perceptronï¼Œç®€ç§°MLPï¼‰ï¼Œä»¥å®ç°å¯¹æ•è·åœºæ™¯çš„æ–°è§†ç‚¹åˆæˆã€‚ç±»ä¼¼åœ°ï¼Œè¿„ä»Šä¸ºæ­¢æœ€é«˜æ•ˆçš„è¾å°„åœºè§£å†³æ–¹æ¡ˆä¹Ÿæ˜¯åŸºäºè¿ç»­è¡¨ç¤ºçš„ï¼Œä¾‹å¦‚åœ¨ä½“ç´ ç½‘æ ¼ï¼ˆå¦‚[Fridovich-Keilå’ŒYuï¼Œ2022]ï¼‰æˆ–å“ˆå¸Œç½‘æ ¼ï¼ˆå¦‚[MÃ¼llerç­‰ï¼Œ2022]ï¼‰ä¸­æ’å€¼å­˜å‚¨çš„å€¼ï¼Œæˆ–è€…åŸºäºç‚¹äº‘ï¼ˆå¦‚[Xuç­‰ï¼Œ2022]ï¼‰ã€‚è™½ç„¶è¿™äº›æ–¹æ³•çš„è¿ç»­æ€§æœ‰åŠ©äºä¼˜åŒ–ï¼Œä½†æ¸²æŸ“æ‰€éœ€çš„éšæœºé‡‡æ ·æˆæœ¬é«˜æ˜‚ï¼Œå¯èƒ½ä¼šäº§ç”Ÿå™ªéŸ³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†ä¸¤è€…çš„ä¼˜ç‚¹ç»“åˆèµ·æ¥ï¼šæˆ‘ä»¬çš„3Dé«˜æ–¯è¡¨ç¤ºåœ¨ä¿æŒæœ€å…ˆè¿›çš„è§†è§‰è´¨é‡å’Œç«äº‰æ€§åŸ¹è®­æ—¶é—´çš„åŒæ—¶ï¼Œæˆ‘ä»¬çš„åŸºäºç“¦ç‰‡çš„å–·æ´’ï¼ˆsplattingï¼‰è§£å†³æ–¹æ¡ˆç¡®ä¿äº†åœ¨1080påˆ†è¾¨ç‡ä¸‹ä»¥æœ€å…ˆè¿›çš„è´¨é‡è¿›è¡Œå®æ—¶æ¸²æŸ“ï¼Œé€‚ç”¨äºå‡ ä¸ªå…ˆå‰å‘å¸ƒçš„æ•°æ®é›†
</details>



<details>
<summary>è‹±æ–‡</summary> 
Our goal is to allow real-time rendering for scenes captured with
 multiple photos, and create the representations with optimization
 times as fast as the most efficient previous methods for typical
 real scenes. Recent methods achieve fast training [Fridovich-Keiland Yu et al. 2022; MÃ¼ller et al. 2022], but struggle to achieve the
 visual quality obtained by the current SOTA NeRF methods, i.e.,
 Mip-NeRF360 [Barron et al. 2022], which requires up to 48 hours of
 training time. The fastâ€“ but lower-qualityâ€“ radiance field methods
 can achieve interactive rendering times depending on the scene
 (10-15 frames per second), but fall short of real-time rendering at
 high resolution.
</details>
 <details>
<summary>ä¸­æ–‡</summary>
æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å®ç°å¯¹ä½¿ç”¨å¤šå¼ ç…§ç‰‡æ•è·çš„åœºæ™¯è¿›è¡Œå®æ—¶æ¸²æŸ“ï¼Œå¹¶åœ¨å…¸å‹çœŸå®åœºæ™¯ä¸­ä»¥ä¸æœ€é«˜æ•ˆçš„å…ˆå‰æ–¹æ³•ç›¸å½“çš„ä¼˜åŒ–æ—¶é—´åˆ›å»ºè¡¨ç¤ºã€‚æœ€è¿‘çš„æ–¹æ³•å®ç°äº†å¿«é€Ÿè®­ç»ƒ1ï¼Œä½†åœ¨è§†è§‰è´¨é‡ä¸Šä»æ— æ³•è¾¾åˆ°å½“å‰SOTA NeRFæ–¹æ³•ï¼ˆä¾‹å¦‚Mip-NeRF360 2ï¼‰ï¼Œåè€…éœ€è¦é•¿è¾¾48å°æ—¶çš„è®­ç»ƒæ—¶é—´ã€‚è™½ç„¶å¿«é€Ÿä½†è´¨é‡è¾ƒä½çš„è¾å°„åœºæ–¹æ³•å¯ä»¥æ ¹æ®åœºæ™¯å®ç°äº¤äº’å¼æ¸²æŸ“æ—¶é—´ï¼ˆæ¯ç§’10-15å¸§ï¼‰ï¼Œä½†æ— æ³•å®ç°é«˜åˆ†è¾¨ç‡ä¸‹çš„å®æ—¶æ¸²æŸ“
</details>

<details>
<summary>è‹±æ–‡</summary>
Our solution builds on three main components. We first intro
duce 3D Gaussians as a flexible and expressive scene representation.
 Westart with the same input as previous NeRF-like methods, i.e.,
 cameras calibrated with Structure-from-Motion (SfM) [Snavely et al.
 2006] and initialize the set of 3D Gaussians with the sparse point
 cloud produced for free as part of the SfM process. In contrast to
 most point-based solutions that require Multi-View Stereo (MVS)
 data [Aliev et al. 2020; Kopanas et al. 2021; RÃ¼ckert et al. 2022], we
 achieve high-quality results with only SfM points as input. Note
 that for the NeRF-synthetic dataset, our method achieves high qual
ity even with random initialization. We show that 3D Gaussians
 are an excellent choice, since they are a differentiable volumetric
 representation, but they can also be rasterized very efficiently by
 projecting them to 2D, and applying standard ğ›¼-blending, using an
 equivalent image formation model as NeRF. The second component
 of our method is optimization of the properties of the 3D Gaussiansâ€“ 3Dposition, opacity ğ›¼, anisotropic covariance, and spherical har
monic (SH) coefficientsâ€“ interleaved with adaptive density control
 steps, where we add and occasionally remove 3D Gaussians during
 optimization. The optimization procedure produces a reasonably
 compact, unstructured, and precise representation of the scene (1-5
 million Gaussians for all scenes tested). The third and final element
 of our method is our real-time rendering solution that uses fast GPU
 sorting algorithms and is inspired by tile-based rasterization, fol
lowing recent work [Lassner and Zollhofer 2021]. However, thanks
 to our 3D Gaussian representation, we can perform anisotropic
 splatting that respects visibility orderingâ€“ thanks to sorting and ğ›¼
blendingâ€“ and enable a fast and accurate backward pass by tracking
 the traversal of as many sorted splats as required.
</details>

<details>
<summary>ä¸­æ–‡</summary>
æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŸºäºä¸‰ä¸ªä¸»è¦ç»„ä»¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†3Dé«˜æ–¯ä½œä¸ºä¸€ç§çµæ´»ä¸”è¡¨è¾¾èƒ½åŠ›å¼ºçš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ã€‚æˆ‘ä»¬ä»ä¸ä¹‹å‰ç±»ä¼¼NeRFæ–¹æ³•ç›¸åŒçš„è¾“å…¥å¼€å§‹ï¼Œå³ä½¿ç”¨**ç»“æ„è¿åŠ¨ï¼ˆStructure-from-Motionï¼Œç®€ç§°SfMï¼‰æ ¡å‡†çš„ç›¸æœºï¼Œå¹¶ä½¿ç”¨SfMè¿‡ç¨‹ä¸­å…è´¹ç”Ÿæˆçš„ç¨€ç–ç‚¹äº‘æ¥åˆå§‹åŒ–3Dé«˜æ–¯é›†åˆã€‚ä¸å¤§å¤šæ•°åŸºäºç‚¹çš„è§£å†³æ–¹æ¡ˆä¸åŒï¼Œåè€…éœ€è¦å¤šè§†å›¾ç«‹ä½“åŒ¹é…ï¼ˆMulti-View Stereoï¼Œç®€ç§°MVSï¼‰**æ•°æ®ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨SfMç‚¹ä½œä¸ºè¾“å…¥å°±å¯ä»¥è·å¾—é«˜è´¨é‡çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨NeRFåˆæˆæ•°æ®é›†ä¸Šï¼Œå³ä½¿éšæœºåˆå§‹åŒ–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå¯ä»¥å®ç°é«˜è´¨é‡çš„ç»“æœã€‚æˆ‘ä»¬è¯æ˜äº†3Dé«˜æ–¯æ˜¯ä¸€ä¸ªå‡ºè‰²çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å¯å¾®åˆ†çš„ä½“ç§¯è¡¨ç¤ºï¼Œä½†ä¹Ÿå¯ä»¥é€šè¿‡å°†å…¶æŠ•å½±åˆ°2Då¹¶åº”ç”¨æ ‡å‡†çš„ğ›¼æ··åˆï¼ˆalpha-blendingï¼‰æ¥é«˜æ•ˆåœ°è¿›è¡Œå…‰æ …åŒ–ï¼Œä½¿ç”¨ä¸NeRFç›¸åŒçš„ç­‰æ•ˆå›¾åƒå½¢æˆæ¨¡å‹ã€‚æˆ‘ä»¬æ–¹æ³•çš„ç¬¬äºŒä¸ªç»„æˆéƒ¨åˆ†æ˜¯å¯¹3Dé«˜æ–¯çš„å±æ€§è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬3Dä½ç½®ã€ä¸é€æ˜åº¦ğ›¼ã€å„å‘å¼‚æ€§åæ–¹å·®å’Œçƒè°ï¼ˆSHï¼‰ç³»æ•°ï¼Œè¿™äº›å±æ€§ä¸è‡ªé€‚åº”å¯†åº¦æ§åˆ¶æ­¥éª¤äº¤æ›¿è¿›è¡Œï¼Œæˆ‘ä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ·»åŠ å¹¶å¶å°”ç§»é™¤3Dé«˜æ–¯ã€‚ä¼˜åŒ–è¿‡ç¨‹äº§ç”Ÿäº†ä¸€ä¸ªåˆç†ç´§å‡‘ã€éç»“æ„åŒ–ä¸”ç²¾ç¡®çš„åœºæ™¯è¡¨ç¤ºï¼ˆåœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­ï¼Œä½¿ç”¨äº†100-500ä¸‡ä¸ªé«˜æ–¯ï¼‰ã€‚æˆ‘ä»¬æ–¹æ³•çš„ç¬¬ä¸‰ä¸ªå’Œæœ€åä¸€ä¸ªå…ƒç´ æ˜¯æˆ‘ä»¬çš„å®æ—¶æ¸²æŸ“è§£å†³æ–¹æ¡ˆï¼Œå®ƒä½¿ç”¨å¿«é€Ÿçš„GPUæ’åºç®—æ³•ï¼Œå—åˆ°åŸºäºç“¦ç‰‡çš„å…‰æ …åŒ–çš„å¯å‘ï¼Œéµå¾ªæœ€è¿‘çš„å·¥ä½œ1ã€‚ç„¶è€Œï¼Œç”±äºæˆ‘ä»¬çš„3Dé«˜æ–¯è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œå„å‘å¼‚æ€§å–·æ´’ï¼Œä»¥éµå®ˆæ’åºå’Œğ›¼æ··åˆï¼ŒåŒæ—¶é€šè¿‡è·Ÿè¸ªæ‰€éœ€æ•°é‡çš„å·²æ’åºå–·æ´’çš„éå†æ¥å®ç°å¿«é€Ÿä¸”å‡†ç¡®çš„åå‘ä¼ é€’ã€‚
</details>
<details>
<summary>è‹±æ–‡</summary>
To summarize, we provide the following contributions:
 â€¢ Theintroductionofanisotropic3DGaussiansasahigh-quality,
 unstructured representation of radiance fields.
 â€¢ An optimization method of 3D Gaussian properties, inter
leaved with adaptive density control that creates high-quality
 representations for captured scenes.
 â€¢ Afast, differentiable rendering approach for the GPU, which
 is visibility-aware, allows anisotropic splatting and fast back
propagation to achieve high-quality novel view synthesis.
 Our results on previously published datasets show that we can opti
mize our 3D Gaussians from multi-view captures and achieve equal
 or better quality than the best quality previous implicit radiance
 f
 ield approaches. We also can achieve training speeds and quality
 similar to the fastest methods and importantly provide the first
 real-time rendering with high quality for novel-view synthesis
</details>

<details>
<summary>ä¸­æ–‡</summary>
æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹è´¡çŒ®ï¼š

<ol>
<li><strong>å¼•å…¥å„å‘å¼‚æ€§3Dé«˜æ–¯</strong>ä½œä¸ºè¾å°„åœºçš„é«˜è´¨é‡ã€éç»“æ„åŒ–è¡¨ç¤ºæ–¹æ³•ã€‚</li>
<li><strong>ä¼˜åŒ–3Dé«˜æ–¯å±æ€§</strong>çš„æ–¹æ³•ï¼Œä¸è‡ªé€‚åº”å¯†åº¦æ§åˆ¶äº¤æ›¿è¿›è¡Œï¼Œä¸ºæ•è·çš„åœºæ™¯åˆ›å»ºé«˜è´¨é‡çš„è¡¨ç¤ºã€‚</li>
<li>åŸºäºGPUçš„<strong>å¿«é€Ÿã€å¯å¾®åˆ†æ¸²æŸ“æ–¹æ³•</strong>ï¼Œå…·æœ‰å¯è§æ€§æ„ŸçŸ¥æ€§ï¼Œæ”¯æŒå„å‘å¼‚æ€§å–·æ´’ï¼Œå¹¶é€šè¿‡è·Ÿè¸ªæ‰€éœ€æ•°é‡çš„å·²æ’åºå–·æ´’å®ç°å¿«é€Ÿä¸”å‡†ç¡®çš„åå‘ä¼ é€’ã€‚</li>
</ol>
<p>æˆ‘ä»¬åœ¨å…ˆå‰å‘å¸ƒçš„æ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬å¯ä»¥ä»å¤šè§†å›¾æ•è·ä¸­ä¼˜åŒ–æˆ‘ä»¬çš„3Dé«˜æ–¯ï¼Œå¹¶è·å¾—ä¸æœ€ä½³éšå¼è¾å°„åœºæ–¹æ³•ç›¸ç­‰æˆ–æ›´å¥½çš„è´¨é‡ã€‚æˆ‘ä»¬è¿˜å¯ä»¥å®ç°ä¸æœ€å¿«æ–¹æ³•ç›¸ä¼¼çš„è®­ç»ƒé€Ÿåº¦å’Œè´¨é‡ï¼Œå¹¶ä¸”é‡è¦çš„æ˜¯é¦–æ¬¡æä¾›äº†å…·æœ‰é«˜è´¨é‡çš„å®æ—¶æ¸²æŸ“çš„æ–°è§†ç‚¹åˆæˆã€‚</p>
</details>

<h3 id="2-RELATEDWORK"><a href="#2-RELATEDWORK" class="headerlink" title="2 RELATEDWORK"></a>2 RELATEDWORK</h3><details>
<summary>è‹±æ–‡</summary>

<p> We first briefly overview traditional reconstruction, then discuss<br> point-based rendering and radiance field work, discussing their  similarity; radiance fields are a vast area, so we focus only ondirectly<br> related work. For complete coverage of the field, please see the<br> excellent recent surveys [Tewari et al. 2022; Xie et al. 2022].</p>
</details>

<details>
<summary>ä¸­æ–‡</summary>
æˆ‘ä»¬é¦–å…ˆç®€è¦å›é¡¾ä¼ ç»Ÿçš„é‡å»ºæ–¹æ³•ï¼Œç„¶åè®¨è®ºåŸºäºç‚¹çš„æ¸²æŸ“å’Œè¾å°„åœºå·¥ä½œï¼Œæ¢è®¨å®ƒä»¬çš„ç›¸ä¼¼æ€§ï¼›è¾å°„åœºæ˜¯ä¸€ä¸ªå¹¿é˜”çš„é¢†åŸŸï¼Œå› æ­¤æˆ‘ä»¬åªå…³æ³¨ç›´æ¥ç›¸å…³çš„å·¥ä½œã€‚è¦å…¨é¢äº†è§£è¯¥é¢†åŸŸï¼Œè¯·å‚é˜…[Tewari et al. 2022; Xie et al. 2022]çš„ä¼˜ç§€æœ€æ–°ç»¼è¿°ã€‚
</details>

<h4 id="2-1-Traditional-Scene-Reconstruction-and-Rendering"><a href="#2-1-Traditional-Scene-Reconstruction-and-Rendering" class="headerlink" title="2.1 Traditional Scene Reconstruction and Rendering"></a>2.1 Traditional Scene Reconstruction and Rendering</h4><details>
<summary>è‹±æ–‡</summary>
 The first novel-view synthesis approaches were based on light fields,
 f
 irst densely sampled [Gortler et al. 1996; Levoy and Hanrahan 1996]
 then allowing unstructured capture [Buehler et al. 2001]. The advent
 of Structure-from-Motion (SfM) [Snavely et al. 2006] enabled an
 entire new domain where a collection of photos could be used to
 synthesize novel views. SfM estimates a sparse point cloud during
 camera calibration, that was initially used for simple visualization
 of 3D space. Subsequent multi-view stereo (MVS) produced im
pressive full 3D reconstruction algorithms over the years [Goesele
 et al. 2007], enabling the development of several view synthesis
 algorithms [Chaurasia et al. 2013; Eisemann et al. 2008; Hedman
 et al. 2018; Kopanas et al. 2021]. All these methods re-project and
 blend the input images into the novel view camera, and use the
 geometry to guide this re-projection. These methods produced ex
cellent results in many cases, but typically cannot completely re
cover from unreconstructed regions, or from â€œover-reconstructionâ€,
 when MVS generates inexistent geometry. Recent neural render
ing algorithms [Tewari et al. 2022] vastly reduce such artifacts and
 avoid the overwhelming cost of storing all input images on the GPU,
 outperforming these methods on most fronts.
</details>

<details>
<summary>ä¸­æ–‡</summary>
ç¬¬ä¸€ä¸ªæ–°é¢–çš„è§†å›¾åˆæˆæ–¹æ³•åŸºäºå…‰åœºï¼Œé¦–å…ˆæ˜¯å¯†é›†é‡‡æ · [Gortler ç­‰äººã€‚ 1996ï¼› Levoy å’Œ Hanrahan 1996] ç„¶åå…è®¸éç»“æ„åŒ–æ•è· [Buehler ç­‰äººã€‚ 2001]ã€‚è¿åŠ¨ç»“æ„ (SfM) çš„å‡ºç° [Snavely ç­‰äººã€‚ 2006]å¯ç”¨äº†ä¸€ä¸ªå…¨æ–°çš„é¢†åŸŸï¼Œå¯ä»¥ä½¿ç”¨ä¸€ç»„ç…§ç‰‡æ¥åˆæˆæ–°é¢–çš„è§†å›¾ã€‚ SfM åœ¨ç›¸æœºæ ¡å‡†æœŸé—´ä¼°è®¡ç¨€ç–ç‚¹äº‘ï¼Œæœ€åˆç”¨äº 3D ç©ºé—´çš„ç®€å•å¯è§†åŒ–ã€‚éšåçš„å¤šè§†å›¾ç«‹ä½“ (MVS) å¤šå¹´æ¥äº§ç”Ÿäº†ä»¤äººå°è±¡æ·±åˆ»çš„å…¨ 3D é‡å»ºç®—æ³• [Goesele ç­‰äººã€‚ 2007]ï¼Œä½¿å¾—å¤šç§è§†å›¾åˆæˆç®—æ³•çš„å¼€å‘æˆä¸ºå¯èƒ½[Chaurasia ç­‰äººã€‚ 2013å¹´ï¼›è‰¾æ–¯æ›¼ç­‰äººã€‚ 2008å¹´ï¼›èµ«å¾·æ›¼ç­‰äººã€‚ 2018ï¼›ç§‘å¸•çº³æ–¯ç­‰äººã€‚ 2021]ã€‚æ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½å°†è¾“å…¥å›¾åƒé‡æ–°æŠ•å½±å¹¶æ··åˆåˆ°æ–°é¢–çš„è§†å›¾ç›¸æœºä¸­ï¼Œå¹¶ä½¿ç”¨å‡ ä½•å½¢çŠ¶æ¥æŒ‡å¯¼è¿™ç§é‡æ–°æŠ•å½±ã€‚è¿™äº›æ–¹æ³•åœ¨è®¸å¤šæƒ…å†µä¸‹äº§ç”Ÿäº†å‡ºè‰²çš„ç»“æœï¼Œä½†å½“ MVS ç”Ÿæˆä¸å­˜åœ¨çš„å‡ ä½•ä½“æ—¶ï¼Œé€šå¸¸æ— æ³•ä»æœªé‡å»ºåŒºåŸŸæˆ–â€œè¿‡åº¦é‡å»ºâ€ä¸­å®Œå…¨æ¢å¤ã€‚æœ€è¿‘çš„ç¥ç»æ¸²æŸ“ç®—æ³• [Tewari ç­‰äººã€‚ 2022]å¤§å¤§å‡å°‘äº†æ­¤ç±»ä¼ªå½±ï¼Œå¹¶é¿å…äº†å°†æ‰€æœ‰è¾“å…¥å›¾åƒå­˜å‚¨åœ¨ GPU ä¸Šçš„å·¨å¤§æˆæœ¬ï¼Œåœ¨å¤§å¤šæ•°æ–¹é¢éƒ½ä¼˜äºè¿™äº›æ–¹æ³•ã€‚
</details>

<h4 id="2-2-Neural-Rendering-and-Radiance-Fields"><a href="#2-2-Neural-Rendering-and-Radiance-Fields" class="headerlink" title="2.2 Neural Rendering and Radiance Fields"></a>2.2 Neural Rendering and Radiance Fields</h4><details>
<summary>è‹±æ–‡</summary>
Deeplearning techniques were adopted early for novel-view synthe
sis [Flynn et al. 2016; Zhou et al. 2016]; CNNs were used to estimate
 blendingweights[Hedmanetal.2018],orfortexture-spacesolutions
 [Riegler and Koltun 2020; Thies et al. 2019]. The use of MVS-based
 geometry is a major drawback of most of these methods; in addition,
 the use of CNNs for final rendering frequently results in temporal
 f
 lickering.
 Volumetric representations for novel-view synthesis were ini
tiated by Soft3D [Penner and Zhang 2017]; deep-learning tech
niques coupled with volumetric ray-marching were subsequently
 proposed[Henzleretal.2019;Sitzmannetal.2019]buildingonacon
tinuous differentiable density field to represent geometry. Rendering
 using volumetric ray-marching has a significant cost due to the large
 number of samples required to query the volume. Neural Radiance
 Fields (NeRFs) [Mildenhall et al. 2020] introduced importance sam
pling and positional encoding to improve quality, but used a large
 Multi-Layer Perceptron negatively affecting speed. The success of
 NeRFhasresulted in anexplosion of follow-up methods that address
 quality and speed, often by introducing regularization strategies; the
 current state-of-the-art in image quality for novel-view synthesis is
 Mip-NeRF360 [Barron et al. 2022]. While the rendering quality is
 outstanding, training and rendering times remain extremely high;
 we are able to equal or in some cases surpass this quality while
 providing fast training and real-time rendering.
</details>

<details>
<summary>ä¸­æ–‡</summary>
æ·±åº¦å­¦ä¹ æŠ€æœ¯æ—©æœŸè¢«ç”¨äºæ–°è§†è§’åˆæˆÂ¹Â²ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¢«ç”¨äºä¼°è®¡èåˆæƒé‡Â³ï¼Œæˆ–è€…ç”¨äºçº¹ç†ç©ºé—´è§£å†³æ–¹æ¡ˆâ´ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™äº›æ–¹æ³•éƒ½ä½¿ç”¨åŸºäºå¤šè§†å›¾ç«‹ä½“å‡ ä½•çš„è¡¨ç¤ºï¼Œè¿™æ˜¯ä¸€ä¸ªä¸»è¦çš„ç¼ºç‚¹ã€‚æ­¤å¤–ï¼Œä½¿ç”¨CNNè¿›è¡Œæœ€ç»ˆæ¸²æŸ“é€šå¸¸ä¼šå¯¼è‡´æ—¶é—´ä¸Šçš„é—ªçƒã€‚

<p>ä½“ç´ è¡¨ç¤ºç”¨äºæ–°è§†è§’åˆæˆçš„åˆå§‹æ–¹æ³•æ˜¯Soft3Dâµã€‚éšåï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸ä½“ç´ å…‰çº¿æŠ•å°„ç›¸ç»“åˆï¼Œæ„å»ºäº†ä¸€ä¸ªè¿ç»­å¯å¾®çš„å¯†åº¦åœºæ¥è¡¨ç¤ºå‡ ä½•å½¢çŠ¶â¶ã€‚ä½¿ç”¨ä½“ç´ å…‰çº¿æŠ•å°„è¿›è¡Œæ¸²æŸ“ç”±äºéœ€è¦å¤§é‡æ ·æœ¬æ¥æŸ¥è¯¢ä½“ç§¯è€Œæˆæœ¬é«˜æ˜‚ã€‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰â· å¼•å…¥äº†é‡è¦æ€§é‡‡æ ·å’Œä½ç½®ç¼–ç ä»¥æé«˜è´¨é‡ï¼Œä½†ä½¿ç”¨äº†å¤§å‹å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œä»è€Œå½±å“äº†é€Ÿåº¦ã€‚NeRFçš„æˆåŠŸå¯¼è‡´äº†ä¸€ç³»åˆ—åç»­æ–¹æ³•çš„æ¶Œç°ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é€šè¿‡å¼•å…¥æ­£åˆ™åŒ–ç­–ç•¥æ¥è§£å†³è´¨é‡å’Œé€Ÿåº¦é—®é¢˜ã€‚ç›®å‰ï¼Œæ–°è§†è§’åˆæˆçš„å›¾åƒè´¨é‡çš„æœ€æ–°æŠ€æœ¯æ˜¯Mip-NeRF360â¸ã€‚è™½ç„¶æ¸²æŸ“è´¨é‡å‡ºè‰²ï¼Œä½†è®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´ä»ç„¶éå¸¸é•¿ã€‚æˆ‘ä»¬èƒ½å¤Ÿåœ¨æä¾›å¿«é€Ÿè®­ç»ƒå’Œå®æ—¶æ¸²æŸ“çš„åŒæ—¶ï¼Œè¾¾åˆ°æˆ–ç”šè‡³è¶…è¶Šè¿™ä¸€è´¨é‡æ°´å¹³ã€‚</p>
</details>

<details>
<summary>è‹±æ–‡</summary>
The most recent methods have focused on faster training and/or
 rendering mostly by exploiting three design choices: the use of spa
tial data structures to store (neural) features that are subsequently
 interpolated during volumetric ray-marching, different encodings,and MLP capacity. Such methods include different variants of space
 discretization [Chen et al. 2022b,a; Fridovich-Keil and Yu et al. 2022;
 Garbin et al. 2021; Hedman et al. 2021; Reiser et al. 2021; Takikawa
 et al. 2021; Wu et al. 2022; Yu et al. 2021], codebooks [Takikawa
 et al. 2022], and encodings such as hash tables [MÃ¼ller et al. 2022],
 allowing the use of a smaller MLP or foregoing neural networks
 completely [Fridovich-Keil and Yu et al. 2022; Sun et al. 2022].
 Mostnotable of these methods are InstantNGP[MÃ¼ller et al. 2022]
 which uses a hash grid and an occupancy grid to accelerate compu
tation and a smaller MLP to represent density and appearance; and
 Plenoxels [Fridovich-Keil and Yu et al. 2022] that use a sparse voxel
 grid to interpolate a continuous density field, and are able to forgo
 neural networks altogether. Both rely on Spherical Harmonics: the
 former to represent directional effects directly, the latter to encode
 its inputs to the color network. While both provide outstanding
 results, these methods can still struggle to represent empty space
 effectively, depending in part on the scene/capture type. In addition,
 image quality is limited in large part by the choice of the structured
 grids used for acceleration, and rendering speed is hindered by the
 need to query many samples for a given ray-marching step. The un
structured, explicit GPU-friendly 3D Gaussians weuseachievefaster
 rendering speed and better quality without neural components.
</details>

<details>
<summary>ä¸­æ–‡</summary>
æœ€è¿‘çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ›´å¿«çš„è®­ç»ƒå’Œ/æˆ–æ¸²æŸ“ï¼Œä¸»è¦é€šè¿‡åˆ©ç”¨ä¸‰ä¸ªè®¾è®¡é€‰æ‹©æ¥å®ç°ï¼šä½¿ç”¨ç©ºé—´æ•°æ®ç»“æ„æ¥å­˜å‚¨ï¼ˆç¥ç»ï¼‰ç‰¹å¾ï¼Œéšååœ¨ä½“ç´ å…‰çº¿æŠ•å°„æœŸé—´è¿›è¡Œæ’å€¼ï¼Œä¸åŒçš„ç¼–ç å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å®¹é‡ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬ä¸åŒå˜ä½“çš„ç©ºé—´ç¦»æ•£åŒ–âµâ¶â·â¸    ï¼Œç æœ¬ï¼Œä»¥åŠå“ˆå¸Œè¡¨ç­‰ç¼–ç ï¼Œå…è®¸ä½¿ç”¨è¾ƒå°çš„MLPæˆ–å®Œå…¨æ”¾å¼ƒç¥ç»ç½‘ç»œ ã€‚
å…¶ä¸­æœ€å€¼å¾—æ³¨æ„çš„æ–¹æ³•åŒ…æ‹¬InstantNGPï¼Œå®ƒä½¿ç”¨å“ˆå¸Œç½‘æ ¼å’Œå ç”¨ç½‘æ ¼æ¥åŠ é€Ÿè®¡ç®—ï¼Œå¹¶ä½¿ç”¨è¾ƒå°çš„MLPæ¥è¡¨ç¤ºå¯†åº¦å’Œå¤–è§‚ï¼›ä»¥åŠPlenoxelsï¼Œå®ƒä½¿ç”¨ç¨€ç–ä½“ç´ ç½‘æ ¼æ¥æ’å€¼è¿ç»­çš„å¯†åº¦åœºï¼Œå¹¶ä¸”èƒ½å¤Ÿå®Œå…¨æ”¾å¼ƒç¥ç»ç½‘ç»œã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½ä¾èµ–äºçƒè°å‡½æ•°ï¼šå‰è€…ç›´æ¥è¡¨ç¤ºæ–¹å‘æ•ˆåº”ï¼Œåè€…å°†å…¶è¾“å…¥ç¼–ç åˆ°é¢œè‰²ç½‘ç»œä¸­ã€‚è™½ç„¶ä¸¤è€…éƒ½æä¾›äº†å‡ºè‰²çš„ç»“æœï¼Œä½†è¿™äº›æ–¹æ³•åœ¨ä¸€å®šç¨‹åº¦ä¸Šä»ç„¶éš¾ä»¥æœ‰æ•ˆåœ°è¡¨ç¤ºç©ºç™½ç©ºé—´ï¼Œè¿™éƒ¨åˆ†å–å†³äºåœºæ™¯/æ•æ‰ç±»å‹ã€‚æ­¤å¤–ï¼Œå›¾åƒè´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°ç”¨äºåŠ é€Ÿçš„ç»“æ„åŒ–ç½‘æ ¼çš„é€‰æ‹©çš„é™åˆ¶ï¼Œè€Œæ¸²æŸ“é€Ÿåº¦å—åˆ°æŸ¥è¯¢ç»™å®šå…‰çº¿æŠ•å°„æ­¥éª¤çš„è®¸å¤šæ ·æœ¬çš„éœ€æ±‚çš„å½±å“ã€‚æˆ‘ä»¬ä½¿ç”¨çš„éç»“æ„åŒ–ã€æ˜ç¡®çš„GPUå‹å¥½çš„3Dé«˜æ–¯å‡½æ•°å®ç°äº†æ›´å¿«çš„æ¸²æŸ“é€Ÿåº¦å’Œæ›´å¥½çš„è´¨é‡ï¼Œè€Œæ— éœ€ç¥ç»ç»„ä»¶ã€‚
</details>

<h4 id="2-3-Point-Based-Rendering-and-Radiance-Fields"><a href="#2-3-Point-Based-Rendering-and-Radiance-Fields" class="headerlink" title="2.3 Point-Based Rendering and Radiance Fields"></a>2.3 Point-Based Rendering and Radiance Fields</h4><details>
<summary>è‹±æ–‡</summary>
Point-based methods efficiently render disconnected and unstruc
tured geometry samples (i.e., point clouds) [Gross and Pfister 2011].
 In its simplest form, point sample rendering [Grossman and Dally
 1998] rasterizes an unstructured set of points with a fixed size, for
 whichit mayexploitnatively supported point types of graphics APIs
 [Sainz and Pajarola 2004] or parallel software rasterization on the
 GPU[Laine and Karras 2011; SchÃ¼tz et al. 2022]. While true to the
 underlying data, point sample rendering suffers from holes, causes
 aliasing, and is strictly discontinuous. Seminal work on high-quality
 point-based rendering addresses these issues by â€œsplattingâ€ point
 primitives with an extent larger than a pixel, e.g., circular or elliptic
 discs, ellipsoids, or surfels [Botsch et al. 2005; Pfister et al. 2000; Ren
 et al. 2002; Zwicker et al. 2001b].
 Therehasbeenrecentinterestindifferentiable point-based render
ing techniques [Wiles et al. 2020; Yifan et al. 2019]. Points have been
 augmented with neural features and rendered using a CNN [Aliev
 et al. 2020; RÃ¼ckert et al. 2022] resulting in fast or even real-time
 view synthesis; however they still depend on MVS for the initial
 geometry, and as such inherit its artifacts, most notably over- or
 under-reconstruction in hard cases such as featureless/shiny areas
 or thin structures.
</details>

<details>
<summary>ä¸­æ–‡</summary>
ç‚¹äº‘æ–¹æ³•é«˜æ•ˆåœ°æ¸²æŸ“äº†ä¸è¿ç»­å’Œéç»“æ„åŒ–çš„å‡ ä½•æ ·æœ¬ï¼ˆå³ç‚¹äº‘ï¼‰Â¹ã€‚åœ¨å…¶æœ€ç®€å•çš„å½¢å¼ä¸­ï¼Œç‚¹æ ·æœ¬æ¸²æŸ“Â²å°†ä¸€ä¸ªéç»“æ„åŒ–çš„ç‚¹é›†è¿›è¡Œå…‰æ …åŒ–ï¼Œå…¶å¤§å°å›ºå®šï¼Œå¯ä»¥åˆ©ç”¨å›¾å½¢APIçš„æœ¬åœ°æ”¯æŒçš„ç‚¹ç±»å‹Â³ï¼Œæˆ–è€…åœ¨GPUä¸Šè¿›è¡Œå¹¶è¡Œè½¯ä»¶å…‰æ …åŒ–â´ã€‚è™½ç„¶ç‚¹æ ·æœ¬æ¸²æŸ“å¿ å®äºåº•å±‚æ•°æ®ï¼Œä½†å®ƒå­˜åœ¨å­”æ´ã€å¯¼è‡´æ··å ï¼Œå¹¶ä¸”ä¸¥æ ¼ä¸è¿ç»­ã€‚å…³äºé«˜è´¨é‡ç‚¹äº‘æ¸²æŸ“çš„å¼€åˆ›æ€§å·¥ä½œé€šè¿‡â€œå–·æ´’â€æ¯”åƒç´ å¤§çš„ç‚¹åŸºå…ƒæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä¾‹å¦‚åœ†å½¢æˆ–æ¤­åœ†å½¢çš„åœ†ç›˜ã€æ¤­çƒä½“æˆ–surfelsâµ   ã€‚

<p>è¿‘å¹´æ¥ï¼Œä¸å¯å¾®åˆ†çš„åŸºäºç‚¹çš„æ¸²æŸ“æŠ€æœ¯å¼•èµ·äº†äººä»¬çš„å…´è¶£ ã€‚ç‚¹è¢«å¢å¼ºä¸ºå…·æœ‰ç¥ç»ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨CNNè¿›è¡Œæ¸²æŸ“ï¼Œä»è€Œå®ç°äº†å¿«é€Ÿç”šè‡³å®æ—¶çš„è§†å›¾åˆæˆï¼›ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶ä¾èµ–äºå¤šè§†å›¾ç«‹ä½“å‡ ä½•ï¼ˆMVSï¼‰æ¥è·å¾—åˆå§‹å‡ ä½•å½¢çŠ¶ï¼Œå¹¶å› æ­¤ç»§æ‰¿äº†å…¶ä¼—æ‰€å‘¨çŸ¥çš„ç¼ºé™·ï¼Œå°¤å…¶æ˜¯åœ¨æ— ç‰¹å¾/é—ªäº®åŒºåŸŸæˆ–è–„ç»“æ„ç­‰å¤æ‚æƒ…å†µä¸‹çš„è¿‡åº¦æˆ–ä¸è¶³é‡å»ºã€‚</p>
</details>

<details>
<summary>ä¸­è‹±æ–‡</summary>
Point-based ğ›¼-blending and NeRF-style volumetric rendering
 share essentially the same image formation model. Specifically, the
 color ğ¶ is given by volumetric rendering along a ray:

<p>ç‚¹äº‘æ–¹æ³•çš„ğ›¼æ··åˆå’ŒNeRFé£æ ¼çš„ä½“ç§¯æ¸²æŸ“å®é™…ä¸Šå…±äº«ç›¸åŒçš„å›¾åƒå½¢æˆæ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œé¢œè‰²ğ¶æ˜¯é€šè¿‡æ²¿ç€ä¸€æ¡å°„çº¿è¿›è¡Œä½“ç§¯æ¸²æŸ“æ¥ç¡®å®šçš„ï¼š</p>
<p>$\ C = \sum_{i=1}^{N} \left( T_i (1 - \exp(-\sigma_i \delta_i))c_i \right)    \quad(1)$<br>å…¶ä¸­ï¼š<br>$ T_i = \exp \left( -\sum_{j=1}^{i-1} \sigma_j \delta_j \right)$</p>
<ul>
<li>$ N $è¡¨ç¤ºæ²¿å°„çº¿çš„æ ·æœ¬æ•°ã€‚</li>
<li>$ T_i $  æ˜¯é€å°„ç‡ã€‚</li>
<li>$ sigma_i $ è¡¨ç¤ºä»‹è´¨çš„å¯†åº¦ã€‚</li>
<li>$ delta_i $è¡¨ç¤ºæ²¿å°„çº¿çš„é—´éš”ã€‚</li>
<li>$ c_i $å¯¹åº”äºæ¯ä¸ªé‡‡æ ·ç‚¹çš„é¢œè‰²ã€‚</li>
</ul>
<p> where samples of density ğœ, transmittanceğ‘‡, and color c are taken<br> along the ray with intervals ğ›¿ğ‘–. This can be re-written as</p>
<p> åœ¨æ²¿ç€å°„çº¿ä»¥é—´éš”ğ›¿ğ‘–å–æ ·å¯†åº¦ğœã€é€å°„ç‡ğ‘‡å’Œé¢œè‰²ğ‘çš„æ ·æœ¬ã€‚è¿™å¯ä»¥é‡æ–°è¡¨è¿°ä¸º</p>
<p>$ \sum_{i=1}^{N} T_i \alpha_i c_i  \quad(2)$</p>
<p>å…¶ä¸­ï¼š</p>
<ul>
<li>$ \alpha_i = (1 - \exp(-\sigma_i \delta_i)) $</li>
<li>$ T_i = \prod_{j=1}^{i-1} (1 - \alpha_i)\ $</li>
</ul>
<p>Atypical neural point-based approach (e.g., [Kopanas et al. 2022,<br> 2021]) computes the colorğ¶ of a pixel by blending N ordered points<br> overlapping the pixel:</p>
<p>å…¸å‹çš„ç¥ç»ç‚¹äº‘æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œ[Kopanasç­‰äººï¼Œ2022å¹´ï¼›2021å¹´]ï¼‰é€šè¿‡æ··åˆé‡å åœ¨åƒç´ ä¸Šçš„Nä¸ªæœ‰åºç‚¹æ¥è®¡ç®—åƒç´ çš„é¢œè‰²ğ¶ï¼š</p>
<p>$ C = \sum_{i \in N} \left( c_i \alpha_i \prod_{j=1}^{i-1} (1 - \alpha_j) \right) \quad(3)$</p>
<p>where cğ‘– is the color of each point and ğ›¼ğ‘– is given by evaluating a<br> 2D Gaussian with covariance Î£ [Yifan et al. 2019] multiplied with a<br> learned per-point opacity.<br>å…¶ä¸­ï¼Œ$c_i$ è¡¨ç¤ºæ¯ä¸ªç‚¹çš„é¢œè‰²ï¼Œ$alpha_i$ æ˜¯é€šè¿‡è¯„ä¼°å…·æœ‰åæ–¹å·® $\Sigma$ çš„äºŒç»´é«˜æ–¯å‡½æ•°å¹¶ä¹˜ä»¥æ¯ä¸ªç‚¹çš„é€æ˜åº¦å¾—å‡ºçš„ã€‚</p>
</details>
<details>
<summary>è‹±æ–‡</summary>
From Eq. 2 and Eq. 3, we can clearly see that the image formation
 model is the same. However, the rendering algorithm is very differ
ent. NeRFs are a continuous representation implicitly representing
 empty/occupied space; expensive random sampling is required to
 f
 ind the samples in Eq. 2 with consequent noise and computational
 expense. In contrast, points are an unstructured, discrete represen
tation that is flexible enough to allow creation, destruction, and
 displacement of geometry similar to NeRF. This is achieved by opti
mizing opacity and positions, as shown by previous work [Kopanas
 et al. 2021], while avoiding the shortcomings of a full volumetric
 representation.
 Pulsar [Lassner and Zollhofer 2021] achieves fast sphere rasteri
zation which inspired our tile-based and sorting renderer. However,
 given the analysis above, we want to maintain (approximate) con
ventional ğ›¼-blending on sorted splats to have the advantages of vol
umetric representations: Our rasterization respects visibility order
 in contrast to their order-independent method. In addition, we back
propagate gradients on all splats in a pixel and rasterize anisotropic
 splats. These elements all contribute to the high visual quality of
 our results (see Sec. 7.3). In addition, previous methods mentioned
 above also use CNNs for rendering, which results in temporal in
stability. Nonetheless, the rendering speed of Pulsar [Lassner and
 Zollhofer 2021] and ADOP[RÃ¼ckertetal.2022]servedasmotivation
 to develop our fast rendering solution.
</details>

<details>
<summary>ä¸­æ–‡</summary>
ä»å…¬å¼2å’Œå…¬å¼3ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°å›¾åƒå½¢æˆæ¨¡å‹æ˜¯ç›¸åŒçš„ã€‚ç„¶è€Œï¼Œæ¸²æŸ“ç®—æ³•å´éå¸¸ä¸åŒã€‚NeRFï¼ˆç¥ç»è¾å°„åœºï¼‰æ˜¯ä¸€ç§è¿ç»­è¡¨ç¤ºï¼Œéšå¼åœ°è¡¨ç¤ºäº†ç©º/å ç”¨ç©ºé—´ï¼›åœ¨å…¬å¼2ä¸­ï¼Œéœ€è¦æ˜‚è´µçš„éšæœºé‡‡æ ·ï¼Œä»è€Œäº§ç”Ÿå™ªéŸ³å’Œè®¡ç®—å¼€é”€ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç‚¹äº‘æ˜¯ä¸€ç§éç»“æ„åŒ–ã€ç¦»æ•£çš„è¡¨ç¤ºï¼Œè¶³å¤Ÿçµæ´»ï¼Œå¯ä»¥å…è®¸ç±»ä¼¼NeRFçš„å‡ ä½•å½¢çŠ¶çš„åˆ›å»ºã€é”€æ¯å’Œä½ç§»ã€‚è¿™æ˜¯é€šè¿‡ä¼˜åŒ–ä¸é€æ˜åº¦å’Œä½ç½®å®ç°çš„ï¼Œæ­£å¦‚ä¹‹å‰çš„å·¥ä½œæ‰€ç¤º[Kopanasç­‰äººï¼Œ2021å¹´]ï¼ŒåŒæ—¶é¿å…äº†å®Œæ•´ä½“ç§¯è¡¨ç¤ºçš„ç¼ºç‚¹ã€‚

<p>Pulsar [Lassnerå’ŒZollhoferï¼Œ2021å¹´]å®ç°äº†å¿«é€Ÿçš„çƒä½“å…‰æ …åŒ–ï¼Œè¿™å¯å‘äº†æˆ‘ä»¬åŸºäºç“¦ç‰‡å’Œæ’åºçš„æ¸²æŸ“å™¨ã€‚ç„¶è€Œï¼Œé‰´äºä¸Šè¿°åˆ†æï¼Œæˆ‘ä»¬å¸Œæœ›ä¿æŒå¯¹æ’åºçš„æ–‘ç‚¹è¿›è¡Œï¼ˆè¿‘ä¼¼ï¼‰å¸¸è§„ğ›¼æ··åˆï¼Œä»¥å…·æœ‰ä½“ç§¯è¡¨ç¤ºçš„ä¼˜åŠ¿ï¼šæˆ‘ä»¬çš„å…‰æ …åŒ–ä¸å…¶æ— åºæ–¹æ³•ç›¸æ¯”å°Šé‡å¯è§æ€§é¡ºåºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨åƒç´ ä¸­åå‘ä¼ æ’­æ‰€æœ‰æ–‘ç‚¹çš„æ¢¯åº¦ï¼Œå¹¶å…‰æ …åŒ–å„å‘å¼‚æ€§æ–‘ç‚¹ã€‚æ‰€æœ‰è¿™äº›å› ç´ éƒ½æœ‰åŠ©äºæˆ‘ä»¬ç»“æœçš„é«˜è§†è§‰è´¨é‡ï¼ˆè¯·å‚é˜…ç¬¬7.3èŠ‚ï¼‰ã€‚æ­¤å¤–ï¼Œä¸Šè¿°æåˆ°çš„å…ˆå‰æ–¹æ³•è¿˜ä½¿ç”¨CNNè¿›è¡Œæ¸²æŸ“ï¼Œè¿™å¯¼è‡´äº†æ—¶é—´ä¸Šçš„ä¸ç¨³å®šæ€§ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒPulsar [Lassnerå’ŒZollhoferï¼Œ2021å¹´]å’ŒADOP[RÃ¼ckertetal.2022]çš„æ¸²æŸ“é€Ÿåº¦æˆä¸ºæˆ‘ä»¬å¼€å‘å¿«é€Ÿæ¸²æŸ“è§£å†³æ–¹æ¡ˆçš„åŠ¨åŠ›ã€‚ğŸŒŸ</p>
</details><details>
<summary>è‹±æ–‡</summary>
While focusing on specular effects, the diffuse point-based ren
dering track of Neural Point Catacaustics [Kopanas et al. 2022]
 overcomes this temporal instability by using an MLP, but still re
quired MVS geometry as input. The most recent method [Zhang
 et al. 2022] in this category does not require MVS, and also uses
 SH for directions; however, it can only handle scenes of one object
 and needs masks for initialization. While fast for small resolutions
 and low point counts, it is unclear how it can scale to scenes of
 typical datasets [Barron et al. 2022; Hedman et al. 2018; Knapitsch
 et al. 2017]. We use 3D Gaussians for a more flexible scene rep
resentation, avoiding the need for MVS geometry and achieving
 real-time rendering thanks to our tile-based rendering algorithm
 for the projected Gaussians.A recent approach [Xu et al. 2022] uses points to represent a
 radiance field with a radial basis function approach. They employ
 point pruning and densification techniques during optimization, but
 use volumetric ray-marching and cannot achieve real-time display
 rates.
 In the domain of human performance capture, 3D Gaussians have
 been used to represent captured human bodies [Rhodin et al. 2015;
 Stoll et al. 2011]; more recently they have been used with volumetric
 ray-marching for vision tasks [Wang et al. 2023]. Neural volumetric
 primitives have been proposed in a similar context [Lombardi et al.
 2021]. While these methods inspired the choice of 3D Gaussians as
 our scene representation, they focus on the specific case of recon
structing and rendering a single isolated object (a human body or
 face), resulting in scenes with small depth complexity. In contrast,
 our optimization of anisotropic covariance, our interleaved optimiza
tion/density control, and efficient depth sorting for rendering allow
 us to handle complete, complex scenes including background, both
 indoors and outdoors and with large depth complexity.
</details>

<details>
<summary>ä¸­æ–‡</summary>
åœ¨å…³æ³¨é•œé¢æ•ˆæœæ—¶ï¼Œç¥ç»ç‚¹å…‰çº¿è¿½è¸ªçš„æ¼«åå°„ç‚¹äº‘æ¸²æŸ“è·Ÿè¸ªï¼ˆä¾‹å¦‚Neural Point Catacaustics [Kopanasç­‰äººï¼Œ2022å¹´]ï¼‰é€šè¿‡ä½¿ç”¨MLPï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰å…‹æœäº†æ—¶é—´ä¸Šçš„ä¸ç¨³å®šæ€§ï¼Œä½†ä»éœ€è¦å¤šè§†å›¾ç«‹ä½“å‡ ä½•ï¼ˆMVSï¼‰ä½œä¸ºè¾“å…¥ã€‚åœ¨è¿™ä¸€ç±»åˆ«ä¸­ï¼Œæœ€è¿‘çš„æ–¹æ³•[Zhangç­‰äººï¼Œ2022å¹´]ä¸éœ€è¦MVSï¼Œè¿˜ä½¿ç”¨äº†çƒè°å‡½æ•°ï¼ˆSHï¼‰æ¥è¡¨ç¤ºæ–¹å‘ï¼›ç„¶è€Œï¼Œå®ƒåªèƒ½å¤„ç†ä¸€ä¸ªç‰©ä½“çš„åœºæ™¯ï¼Œå¹¶ä¸”éœ€è¦ç”¨äºåˆå§‹åŒ–çš„é®ç½©ã€‚è™½ç„¶å¯¹äºå°åˆ†è¾¨ç‡å’Œä½ç‚¹æ•°çš„æƒ…å†µä¸‹é€Ÿåº¦å¾ˆå¿«ï¼Œä½†å®ƒå°šä¸æ¸…æ¥šå¦‚ä½•æ‰©å±•åˆ°å…¸å‹æ•°æ®é›†çš„åœºæ™¯[Barronç­‰äººï¼Œ2022å¹´ï¼›Hedmanç­‰äººï¼Œ2018å¹´ï¼›Knapitschç­‰äººï¼Œ2017å¹´]ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç»´é«˜æ–¯å‡½æ•°æ¥å®ç°æ›´çµæ´»çš„åœºæ™¯è¡¨ç¤ºï¼Œé¿å…äº†å¯¹MVSå‡ ä½•çš„éœ€æ±‚ï¼Œå¹¶é€šè¿‡æˆ‘ä»¬çš„åŸºäºç“¦ç‰‡çš„æ¸²æŸ“ç®—æ³•å®ç°äº†æŠ•å½±é«˜æ–¯å‡½æ•°çš„å®æ—¶æ¸²æŸ“ã€‚

<p>æœ€è¿‘çš„ä¸€ç§æ–¹æ³•[Xuç­‰äººï¼Œ2022å¹´]ä½¿ç”¨ç‚¹æ¥è¡¨ç¤ºå…·æœ‰å¾„å‘åŸºå‡½æ•°æ–¹æ³•çš„è¾å°„åœºã€‚ä»–ä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é‡‡ç”¨äº†ç‚¹ä¿®å‰ªå’ŒåŠ å¯†æŠ€æœ¯ï¼Œä½†ä½¿ç”¨äº†ä½“ç§¯å…‰çº¿è¡Œè¿›ï¼Œå¹¶ä¸”æ— æ³•å®ç°å®æ—¶æ˜¾ç¤ºé€Ÿç‡ã€‚</p>
<p>åœ¨äººä½“æ€§èƒ½æ•æ‰é¢†åŸŸï¼Œä¸‰ç»´é«˜æ–¯å‡½æ•°å·²è¢«ç”¨äºè¡¨ç¤ºæ•æ‰åˆ°çš„äººä½“[Rhodinç­‰äººï¼Œ2015å¹´ï¼›Stollç­‰äººï¼Œ2011å¹´]ï¼›æœ€è¿‘ï¼Œå®ƒä»¬å·²ä¸ä½“ç§¯å…‰çº¿è¡Œè¿›ä¸€èµ·ç”¨äºè§†è§‰ä»»åŠ¡[Wangç­‰äººï¼Œ2023å¹´]ã€‚ç¥ç»ä½“ç§¯åŸè¯­åœ¨ç±»ä¼¼çš„èƒŒæ™¯ä¸‹ä¹Ÿè¢«æå‡º[Lombardiç­‰äººï¼Œ2021å¹´]ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å¯å‘äº†æˆ‘ä»¬é€‰æ‹©ä¸‰ç»´é«˜æ–¯å‡½æ•°ä½œä¸ºåœºæ™¯è¡¨ç¤ºï¼Œä½†å®ƒä»¬ä¸“æ³¨äºé‡å»ºå’Œæ¸²æŸ“å•ä¸ªå­¤ç«‹å¯¹è±¡ï¼ˆä¾‹å¦‚äººä½“æˆ–é¢éƒ¨ï¼‰ï¼Œä»è€Œå¯¼è‡´æ·±åº¦å¤æ‚åº¦è¾ƒå°çš„åœºæ™¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¯¹å„å‘å¼‚æ€§åæ–¹å·®çš„ä¼˜åŒ–ã€äº¤é”™çš„ä¼˜åŒ–/å¯†åº¦æ§åˆ¶ä»¥åŠç”¨äºæ¸²æŸ“çš„é«˜æ•ˆæ·±åº¦æ’åºä½¿æˆ‘ä»¬èƒ½å¤Ÿå¤„ç†å®Œæ•´ã€å¤æ‚çš„åœºæ™¯ï¼ŒåŒ…æ‹¬å®¤å†…å¤–èƒŒæ™¯ï¼Œä»¥åŠå…·æœ‰å¤§æ·±åº¦å¤æ‚åº¦çš„åœºæ™¯ã€‚</p>
</details>

<h3 id="3-OVERVIEW"><a href="#3-OVERVIEW" class="headerlink" title="3 OVERVIEW"></a>3 OVERVIEW</h3><details>
<summary>è‹±æ–‡</summary>
The input to our method is a set of images of a static scene, together
 with the corresponding cameras calibrated by SfM [SchÃ¶nberger
 and Frahm 2016] which produces a sparse point cloud as a side
effect. From these points we create a set of 3D Gaussians (Sec. 4),
 defined by a position (mean), covariance matrix and opacity ğ›¼, that
 allows a very flexible optimization regime. This results in a reason
ably compact representation of the 3D scene, in part because highly
 anisotropic volumetric splats can be used to represent fine structures
 compactly. The directional appearance component (color) of the
 radiance field is represented via spherical harmonics (SH), following
 standard practice [Fridovich-Keil and Yu et al. 2022; MÃ¼ller et al.
 2022]. Our algorithm proceeds to create the radiance field represen
tation (Sec. 5) via a sequence of optimization steps of 3D Gaussian
 parameters, i.e., position, covariance, ğ›¼ and SH coefficients inter
leaved with operations for adaptive control of the Gaussian density.
 The key to the efficiency of our method is our tile-based rasterizer
 (Sec. 6) that allows ğ›¼-blending of anisotropic splats, respecting visi
bility order thanks to fast sorting. Out fast rasterizer also includes
 a fast backward pass by tracking accumulated ğ›¼ values, without a
 limit on the number of Gaussians that can receive gradients. The
 overview of our method is illustrated in Fig. 2.
</details>

<details>
<summary>ä¸­æ–‡</summary>
æˆ‘ä»¬æ–¹æ³•çš„è¾“å…¥æ˜¯ä¸€ç»„é™æ€åœºæ™¯çš„å›¾åƒï¼Œä»¥åŠç”±SfMï¼ˆç»“æ„å…‰æŸæ³•ï¼‰æ ¡å‡†çš„ç›¸åº”ç›¸æœºï¼Œè¿™ä¼šäº§ç”Ÿä¸€ä¸ªç¨€ç–çš„ç‚¹äº‘ä½œä¸ºå‰¯ä½œç”¨ã€‚ä»è¿™äº›ç‚¹ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ç»„ä¸‰ç»´é«˜æ–¯å‡½æ•°ï¼ˆç¬¬4èŠ‚ï¼‰ï¼Œç”±ä½ç½®ï¼ˆå‡å€¼ï¼‰ã€åæ–¹å·®çŸ©é˜µå’Œä¸é€æ˜åº¦ğ›¼å®šä¹‰ï¼Œä»è€Œå®ç°äº†éå¸¸çµæ´»çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚è¿™å¯¼è‡´äº†å¯¹3Dåœºæ™¯çš„ç›¸å¯¹ç´§å‡‘çš„è¡¨ç¤ºï¼Œéƒ¨åˆ†åŸå› æ˜¯é«˜åº¦å„å‘å¼‚æ€§çš„ä½“ç§¯æ–‘ç‚¹å¯ä»¥ç´§å‡‘åœ°è¡¨ç¤ºç²¾ç»†ç»“æ„ã€‚è¾å°„åœºçš„æ–¹å‘å¤–è§‚æˆåˆ†ï¼ˆé¢œè‰²ï¼‰é€šè¿‡çƒè°å‡½æ•°ï¼ˆSHï¼‰è¡¨ç¤ºï¼Œéµå¾ªæ ‡å‡†åšæ³•[Fridovich-Keilå’ŒYuç­‰äººï¼Œ2022å¹´ï¼›MÃ¼llerç­‰äººï¼Œ2022å¹´]ã€‚æˆ‘ä»¬çš„ç®—æ³•é€šè¿‡ä¸€ç³»åˆ—å¯¹3Dé«˜æ–¯å‚æ•°çš„ä¼˜åŒ–æ­¥éª¤ï¼ˆå³ä½ç½®ã€åæ–¹å·®ã€ğ›¼å’ŒSHç³»æ•°ï¼‰æ¥åˆ›å»ºè¾å°„åœºè¡¨ç¤ºï¼ˆç¬¬5èŠ‚ï¼‰ï¼Œå¹¶ä¸è‡ªé€‚åº”æ§åˆ¶é«˜æ–¯å¯†åº¦çš„æ“ä½œäº¤é”™è¿›è¡Œã€‚æˆ‘ä»¬æ–¹æ³•æ•ˆç‡çš„å…³é”®åœ¨äºæˆ‘ä»¬åŸºäºç“¦ç‰‡çš„å…‰æ …åŒ–å™¨ï¼ˆç¬¬6èŠ‚ï¼‰ï¼Œå®ƒå…è®¸å„å‘å¼‚æ€§æ–‘ç‚¹çš„ğ›¼æ··åˆï¼Œé€šè¿‡å¿«é€Ÿæ’åºå°Šé‡å¯è§æ€§é¡ºåºã€‚æˆ‘ä»¬çš„å¿«é€Ÿå…‰æ …åŒ–å™¨è¿˜åŒ…æ‹¬é€šè¿‡è·Ÿè¸ªç´¯ç§¯çš„ğ›¼å€¼è¿›è¡Œå¿«é€Ÿåå‘ä¼ æ’­ï¼Œè€Œä¸é™åˆ¶å¯ä»¥æ¥æ”¶æ¢¯åº¦çš„é«˜æ–¯æ•°é‡ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ¦‚è¿°å¦‚å›¾2æ‰€ç¤ºã€‚
</details>

<h3 id="4-DIFFERENTIABLE-3D-GAUSSIAN-SPLATTING"><a href="#4-DIFFERENTIABLE-3D-GAUSSIAN-SPLATTING" class="headerlink" title="4 DIFFERENTIABLE 3D GAUSSIAN SPLATTING"></a>4 DIFFERENTIABLE 3D GAUSSIAN SPLATTING</h3><details>
<summary>ä¸­è‹±æ–‡</summary>
 Our goal is to optimize a scene representation that allows high
quality novel view synthesis, starting from a sparse set of (SfM)
 points without normals. To do this, we need a primitive that inherits
 the properties of differentiable volumetric representations, while
 at the same time being unstructured and explicit to allow very fast
 rendering. We choose 3D Gaussians, which are differentiable and
 can be easily projected to 2D splats allowing fast ğ›¼-blending for
 rendering.
 Our representation has similarities to previous methods that use
 2D points [Kopanas et al. 2021; Yifan et al. 2019] and assume each
 point is a small planar circle with a normal. Given the extreme
 sparsity of SfM points it is very hard to estimate normals. Similarly, optimizing very noisy normals from such an estimation would be
 very challenging. Instead, we model the geometry as a set of 3D
 Gaussians that do not require normals. Our Gaussians are defined
 by a full 3D covariance matrix Î£ defined in world space [Zwicker
 et al. 2001a] centered at point (mean) ğœ‡:

<p> æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼˜åŒ–ä¸€ç§åœºæ™¯è¡¨ç¤ºï¼Œå…è®¸ä»ç¨€ç–çš„ï¼ˆSfMï¼‰ç‚¹é›†å¼€å§‹è¿›è¡Œé«˜è´¨é‡çš„æ–°è§†è§’åˆæˆï¼Œè€Œæ— éœ€æ³•çº¿ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ—¢å…·æœ‰å¯å¾®åˆ†ä½“ç§¯è¡¨ç¤ºçš„ç‰¹æ€§ï¼ŒåŒæ—¶åˆæ˜¯éç»“æ„åŒ–ä¸”æ˜ç¡®çš„ï¼Œä»¥ä¾¿å®ç°éå¸¸å¿«é€Ÿçš„æ¸²æŸ“ã€‚æˆ‘ä»¬é€‰æ‹©äº†ä¸‰ç»´é«˜æ–¯å‡½æ•°ï¼Œå®ƒä»¬æ˜¯å¯å¾®åˆ†çš„ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾æŠ•å½±åˆ°äºŒç»´æ–‘ç‚¹ï¼Œä»è€Œå®ç°äº†å¿«é€Ÿçš„ğ›¼æ··åˆæ¸²æŸ“ã€‚</p>
<p>æˆ‘ä»¬çš„è¡¨ç¤ºä¸ä¹‹å‰ä½¿ç”¨2Dç‚¹çš„æ–¹æ³•[Kopanasç­‰äººï¼Œ2021å¹´ï¼›Yifanç­‰äººï¼Œ2019å¹´]æœ‰ç›¸ä¼¼ä¹‹å¤„ï¼Œå¹¶å‡è®¾æ¯ä¸ªç‚¹æ˜¯ä¸€ä¸ªå¸¦æœ‰æ³•çº¿çš„å°å¹³é¢åœ†ã€‚é‰´äºSfMç‚¹çš„æç«¯ç¨€ç–æ€§ï¼Œå¾ˆéš¾ä¼°è®¡æ³•çº¿ã€‚ç±»ä¼¼åœ°ï¼Œä»è¿™ç§ä¼°è®¡ä¸­ä¼˜åŒ–éå¸¸å˜ˆæ‚çš„æ³•çº¿å°†æ˜¯éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å‡ ä½•å½¢çŠ¶å»ºæ¨¡ä¸ºä¸€ç»„ä¸éœ€è¦æ³•çº¿çš„ä¸‰ç»´é«˜æ–¯å‡½æ•°ã€‚æˆ‘ä»¬çš„é«˜æ–¯å‡½æ•°ç”±åœ¨ä¸–ç•Œç©ºé—´ä¸­å®šä¹‰çš„å®Œæ•´ä¸‰ç»´åæ–¹å·®çŸ©é˜µÎ£å®šä¹‰ï¼Œä»¥ç‚¹ï¼ˆå‡å€¼ï¼‰ğœ‡ä¸ºä¸­å¿ƒ[Zwickerç­‰äººï¼Œ2001aå¹´]ï¼š</p>
<p>$G(x) = e^{-\frac{1}{2} (x)^T \Sigma^{-1} (x)}\quad(4)$</p>
<p>This Gaussian function is multiplied by $\alpha$ in our blending process.<br>è¿™ä¸ªé«˜æ–¯å‡½æ•°åœ¨æˆ‘ä»¬çš„æ··åˆè¿‡ç¨‹ä¸­ä¸ $\alpha$ ç›¸ä¹˜</p>
<p>However,we need to project our 3D Gaussians to 2D for rendering.<br>Zwicker et al. [2001a] demonstrate how to do this projection to image space. Given a viewing transformation ğ‘Š the covariance matrix Î£â€² in camera coordinates is given as follows:</p>
<p>ç„¶è€Œï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„ä¸‰ç»´é«˜æ–¯å‡½æ•°æŠ•å½±åˆ°äºŒç»´ä»¥è¿›è¡Œæ¸²æŸ“ã€‚Zwickerç­‰äºº[2001a]æ¼”ç¤ºäº†å¦‚ä½•å°†å…¶æŠ•å½±åˆ°å›¾åƒç©ºé—´ã€‚ç»™å®šä¸€ä¸ªè§†å›¾å˜æ¢ğ‘Šï¼Œåæ–¹å·®çŸ©é˜µÎ£â€²åœ¨ç›¸æœºåæ ‡ä¸­å®šä¹‰å¦‚ä¸‹ï¼š</p>
<p> $ \Sigmaâ€™ = J W \Sigma W^T J^T \quad(5)$</p>
<p> where ğ½ is the Jacobian of the affine approximation of the projective<br> transformation. Zwicker et al. [2001a] also show that if we skip the<br> third row and column of Î£â€², we obtain a 2Ã—2 variance matrix with<br> the same structure and properties as if we would start from planar<br> points with normals, as in previous work [Kopanas et al. 2021].<br> å…¶ä¸­ï¼Œğ½æ˜¯æŠ•å½±å˜æ¢çš„ä»¿å°„è¿‘ä¼¼çš„é›…å¯æ¯”çŸ©é˜µã€‚Zwickerç­‰äºº[2001a]è¿˜è¡¨æ˜ï¼Œå¦‚æœæˆ‘ä»¬è·³è¿‡Î£â€²çš„ç¬¬ä¸‰è¡Œå’Œç¬¬ä¸‰åˆ—ï¼Œæˆ‘ä»¬å°†è·å¾—ä¸€ä¸ª2Ã—2çš„æ–¹å·®çŸ©é˜µï¼Œå…¶ç»“æ„å’Œæ€§è´¨ä¸æˆ‘ä»¬ä»å…·æœ‰æ³•çº¿çš„å¹³é¢ç‚¹å¼€å§‹çš„æƒ…å†µç›¸åŒï¼Œå°±åƒä¹‹å‰çš„å·¥ä½œ[Kopanasç­‰äººï¼Œ2021å¹´]ä¸€æ ·<br> An obvious approach would be to directly optimize the covariance<br> matrix Î£ to obtain 3D Gaussians that represent the radiance field.<br> However, covariance matrices have physical meaning only when<br> they are positive semi-definite. For our optimization of all our pa<br>rameters, we use gradient descent that cannot be easily constrained<br> to produce such valid matrices, and update steps and gradients can<br> very easily create invalid covariance matrices.<br> ä¸€ä¸ªæ˜æ˜¾çš„æ–¹æ³•æ˜¯ç›´æ¥ä¼˜åŒ–åæ–¹å·®çŸ©é˜µÎ£ï¼Œä»¥è·å¾—è¡¨ç¤ºè¾å°„åœºçš„ä¸‰ç»´é«˜æ–¯å‡½æ•°ã€‚ç„¶è€Œï¼Œåæ–¹å·®çŸ©é˜µåªæœ‰åœ¨å®ƒä»¬æ˜¯åŠæ­£å®šçš„æ—¶å€™æ‰å…·æœ‰ç‰©ç†æ„ä¹‰ã€‚å¯¹äºæˆ‘ä»¬æ‰€æœ‰å‚æ•°çš„ä¼˜åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼Œå¾ˆéš¾çº¦æŸå…¶äº§ç”Ÿè¿™æ ·æœ‰æ•ˆçš„çŸ©é˜µï¼Œè€Œä¸”æ›´æ–°æ­¥éª¤å’Œæ¢¯åº¦å¾ˆå®¹æ˜“äº§ç”Ÿæ— æ•ˆçš„åæ–¹å·®çŸ©é˜µã€‚<br> As a result, we opted for a more intuitive, yet equivalently ex<br>pressive representation for optimization. The covariance matrix Î£<br> of a 3D Gaussian is analogous to describing the configuration of an<br> ellipsoid. Given a scaling matrix ğ‘† and rotation matrix ğ‘…, we can<br> f<br> ind the corresponding Î£:<br> å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ç§æ›´ç›´è§‚ä½†åŒæ ·è¡¨è¾¾èƒ½åŠ›çš„ä¼˜åŒ–è¡¨ç¤ºã€‚ä¸‰ç»´é«˜æ–¯å‡½æ•°çš„åæ–¹å·®çŸ©é˜µÎ£ç±»ä¼¼äºæè¿°æ¤­çƒä½“çš„é…ç½®ã€‚ç»™å®šä¸€ä¸ªç¼©æ”¾çŸ©é˜µğ‘†å’Œæ—‹è½¬çŸ©é˜µğ‘…ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ç›¸åº”çš„Î£ï¼š<br> $ \Sigma = R S S^T R^T \quad(6) $<br> To allow independent optimization of both factors, we store them<br> separately: a 3D vector ğ‘  for scaling and a quaternion ğ‘ to represent<br> rotation. These canbetrivially converted to their respective matrices<br> and combined, making sure to normalize ğ‘ to obtain a valid unit<br> quaternion.<br> ä¸ºäº†å…è®¸ç‹¬ç«‹ä¼˜åŒ–è¿™ä¸¤ä¸ªå› ç´ ï¼Œæˆ‘ä»¬å°†å®ƒä»¬åˆ†å¼€å­˜å‚¨ï¼šä¸€ä¸ªç”¨äºç¼©æ”¾çš„ä¸‰ç»´å‘é‡ğ‘ å’Œä¸€ä¸ªç”¨äºè¡¨ç¤ºæ—‹è½¬çš„å››å…ƒæ•°ğ‘ã€‚è¿™äº›å¯ä»¥è½»æ¾åœ°è½¬æ¢ä¸ºå„è‡ªçš„çŸ©é˜µå¹¶ç»„åˆï¼Œç¡®ä¿å½’ä¸€åŒ–ğ‘ä»¥è·å¾—æœ‰æ•ˆçš„å•ä½å››å…ƒæ•°ã€‚<br> To avoid significant overhead due to automatic differentiation<br> during training, we derive the gradients for all parameters explicitly.<br> Details of the exact derivative computations are in appendix A.<br> ä¸ºäº†é¿å…ç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„è‡ªåŠ¨å¾®åˆ†è€Œäº§ç”Ÿçš„æ˜¾è‘—å¼€é”€ï¼Œæˆ‘ä»¬æ˜ç¡®åœ°è®¡ç®—äº†æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚å…³äºç²¾ç¡®å¯¼æ•°è®¡ç®—çš„è¯¦ç»†ä¿¡æ¯è¯·å‚è§é™„å½•Aã€‚<br> This representation of anisotropic covarianceâ€“ suitable for op<br>timizationâ€“ allows us to optimize 3D Gaussians to adapt to the<br> geometry of different shapes in captured scenes, resulting in a fairly<br> compact representation. Fig. 3 illustrates such cases.<br> è¿™ç§é€‚ç”¨äºä¼˜åŒ–çš„å„å‘å¼‚æ€§åæ–¹å·®è¡¨ç¤ºä½¿æˆ‘ä»¬èƒ½å¤Ÿä¼˜åŒ–ä¸‰ç»´é«˜æ–¯å‡½æ•°ä»¥é€‚åº”æ•è·åœºæ™¯ä¸­ä¸åŒå½¢çŠ¶çš„å‡ ä½•ç»“æ„ï¼Œä»è€Œå¾—åˆ°ç›¸å¯¹ç´§å‡‘çš„è¡¨ç¤ºã€‚å›¾3è¯´æ˜äº†è¿™æ ·çš„æƒ…å†µã€‚</p>
</details>

<h4 id="5-OPTIMIZATION-WITH-ADAPTIVE-DENSITY-CONTROL-OF-3D-GAUSSIANS"><a href="#5-OPTIMIZATION-WITH-ADAPTIVE-DENSITY-CONTROL-OF-3D-GAUSSIANS" class="headerlink" title="5 OPTIMIZATION WITH ADAPTIVE DENSITY CONTROL OF 3D GAUSSIANS"></a>5 OPTIMIZATION WITH ADAPTIVE DENSITY CONTROL OF 3D GAUSSIANS</h4><p><img src="/2024/03/30/3dgs/fig2.png" alt="Fig 2"><br>å›¾2. ä¼˜åŒ–ä»ç¨€ç–çš„SfMç‚¹äº‘å¼€å§‹ï¼Œåˆ›å»ºäº†ä¸€ç»„3Dé«˜æ–¯å‡½æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹è¿™ç»„é«˜æ–¯å‡½æ•°çš„å¯†åº¦è¿›è¡Œä¼˜åŒ–å’Œè‡ªé€‚åº”æ§åˆ¶ã€‚åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¿«é€Ÿçš„åŸºäºç“¦ç‰‡çš„æ¸²æŸ“å™¨ï¼Œä½¿å…¶åœ¨è®­ç»ƒæ—¶é—´ä¸Šå…·æœ‰ä¸SOTAå¿«é€Ÿè¾å°„åœºæ–¹æ³•ç›¸ç«äº‰çš„ä¼˜åŠ¿ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬çš„æ¸²æŸ“å™¨å…è®¸å¯¹å„ç§åœºæ™¯è¿›è¡Œå®æ—¶å¯¼èˆª.<br>Projectionï¼šæŠ•å°„ï¼›Adaptive Density Controlï¼šè‡ªé€‚åº”å¯†åº¦æ§åˆ¶ï¼›å¯å¾®åˆ†ç“¦ç‰‡å…‰æ …åŒ–å™¨ï¼ˆDifferentiable Tile Rasterizerï¼‰ï¼›æ“ä½œæµç¨‹ï¼ˆOperation Flowï¼‰ï¼›æ¢¯åº¦æµï¼ˆGradient Flowï¼‰</p>
<p><img src="/2024/03/30/3dgs/fig3.png" alt="Fig 3"><br>å›¾3ã€‚æˆ‘ä»¬åœ¨ä¼˜åŒ–åå°†3Dé«˜æ–¯å‡½æ•°ç¼©å°60%ï¼ˆæœ€å³ä¾§ï¼‰ã€‚è¿™æ¸…æ¥šåœ°å±•ç¤ºäº†ç»è¿‡ä¼˜åŒ–åç´§å‡‘åœ°è¡¨ç¤ºå¤æ‚å‡ ä½•å½¢çŠ¶çš„3Dé«˜æ–¯å‡½æ•°çš„å„å‘å¼‚æ€§å½¢çŠ¶ã€‚å·¦ä¾§æ˜¯å®é™…æ¸²æŸ“çš„å›¾åƒã€‚</p>
<details>
<summary>è‹±æ–‡</summary>
The core of our approach is the optimization step, which creates
 a dense set of 3D Gaussians accurately representing the scene for
 free-view synthesis. In addition to positions ğ‘, ğ›¼, and covariance
 Î£, we also optimize SH coefficients representing color ğ‘ of each
 Gaussian to correctly capture the view-dependent appearance of
 the scene. The optimization of these parameters is interleaved with
 steps that control the density of the Gaussians to better represent
 the scene.
</details>
<details>
<summary>ä¸­æ–‡</summary>
æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¼˜åŒ–æ­¥éª¤ï¼Œå®ƒåˆ›å»ºäº†ä¸€ç»„å¯†é›†çš„ä¸‰ç»´é«˜æ–¯å‡½æ•°ï¼Œå‡†ç¡®åœ°è¡¨ç¤ºäº†ç”¨äºè‡ªç”±è§†è§’åˆæˆçš„åœºæ™¯ã€‚é™¤äº†ä½ç½®ğ‘ã€ğ›¼å’Œåæ–¹å·®Î£ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ä¼˜åŒ–äº†è¡¨ç¤ºæ¯ä¸ªé«˜æ–¯å‡½æ•°é¢œè‰²ğ‘çš„çƒè°ç³»æ•°ï¼Œä»¥æ­£ç¡®æ•æ‰åœºæ™¯çš„è§†è§’ç›¸å…³å¤–è§‚ã€‚è¿™äº›å‚æ•°çš„ä¼˜åŒ–ä¸æ§åˆ¶é«˜æ–¯å‡½æ•°å¯†åº¦çš„æ­¥éª¤äº¤é”™è¿›è¡Œï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤ºåœºæ™¯ã€‚
</details>

<h4 id="5-1-Optimization"><a href="#5-1-Optimization" class="headerlink" title="5.1 Optimization"></a>5.1 Optimization</h4><details>
<summary>è‹±æ–‡</summary>
The optimization is based on successive iterations of rendering and
 comparing the resulting image to the training views in the captured
 dataset. Inevitably, geometry may be incorrectly placed due to the
 ambiguities of 3D to 2D projection. Our optimization thus needs to
 be able to create geometry and also destroy or move geometry if it
 has been incorrectly positioned. The quality of the parameters of the
 covariances of the 3D Gaussians is critical for the compactness of
 the representation since large homogeneous areas can be captured
 with a small number of large anisotropic Gaussians.
 Weuse Stochastic Gradient Descent techniques for optimization,
 taking full advantage of standard GPU-accelerated frameworks,
 and the ability to add custom CUDA kernels for some operations,
 following recent best practice [Fridovich-Keil and Yu et al. 2022;
 Sun et al. 2022]. In particular, our fast rasterization (see Sec. 6) is
 critical in the efficiency of our optimization, since it is the main
 computational bottleneck of the optimization
</details>
<details>
<summary>ä¸­æ–‡</summary>
ä¼˜åŒ–åŸºäºè¿ç»­çš„æ¸²æŸ“å’Œå°†ç”Ÿæˆçš„å›¾åƒä¸æ•è·æ•°æ®é›†ä¸­çš„è®­ç»ƒè§†å›¾è¿›è¡Œæ¯”è¾ƒçš„è¿­ä»£ã€‚ç”±äº3Dåˆ°2DæŠ•å½±çš„ä¸ç¡®å®šæ€§ï¼Œå‡ ä½•ä½“å¯èƒ½è¢«é”™è¯¯åœ°æ”¾ç½®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ä¼˜åŒ–éœ€è¦èƒ½å¤Ÿåˆ›å»ºå‡ ä½•ä½“ï¼ŒåŒæ—¶å¦‚æœå…¶ä½ç½®ä¸æ­£ç¡®ï¼Œè¿˜å¯ä»¥é”€æ¯æˆ–ç§»åŠ¨å‡ ä½•ä½“ã€‚3Dé«˜æ–¯å‡½æ•°çš„åæ–¹å·®å‚æ•°çš„è´¨é‡å¯¹äºè¡¨ç¤ºçš„ç´§å‡‘æ€§è‡³å…³é‡è¦ï¼Œå› ä¸ºå¤§å‡åŒ€åŒºåŸŸå¯ä»¥ç”¨å°‘é‡å¤§å‹å„å‘å¼‚æ€§é«˜æ–¯å‡½æ•°æ•è·ã€‚

<p>æˆ‘ä»¬ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨æ ‡å‡†çš„GPUåŠ é€Ÿæ¡†æ¶ï¼Œå¹¶ä¸”å¯ä»¥ä¸ºæŸäº›æ“ä½œæ·»åŠ è‡ªå®šä¹‰çš„CUDAå†…æ ¸ï¼Œéµå¾ªæœ€è¿‘çš„æœ€ä½³å®è·µÂ¹Â²ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¿«é€Ÿçš„å…‰æ …åŒ–ï¼ˆè§ç¬¬6èŠ‚ï¼‰å¯¹äºæˆ‘ä»¬çš„ä¼˜åŒ–æ•ˆç‡è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒæ˜¯ä¼˜åŒ–çš„ä¸»è¦è®¡ç®—ç“¶é¢ˆã€‚</p>
</details>

<details>
<summary>è‹±æ–‡</summary>
</details>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">å°ä¹ä¹</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://zhenwusi.github.io/2024/03/30/3dgs/">https://zhenwusi.github.io/2024/03/30/3dgs/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/about" target="_blank">å°ä¹ä¹</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/3dgs/">
                                    <span class="chip bg-color">3dgs</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">èµ</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">ä½ çš„èµè¯†æ˜¯æˆ‘å‰è¿›çš„åŠ¨åŠ›</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">æ”¯ä»˜å®</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">å¾® ä¿¡</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/null" class="reward-img" alt="æ”¯ä»˜å®æ‰“èµäºŒç»´ç ">
                    </div>
                    <div id="wechat">
                        <img src="/null" class="reward-img" alt="å¾®ä¿¡æ‰“èµäºŒç»´ç ">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;æœ¬ç¯‡
            </div>
            <div class="card">
                <a href="/2024/03/30/3dgs/">
                    <div class="card-image">
                        
                        <img src="/%5Cimg%5C2.jpg" class="responsive-img" alt="3d gaussian splating">
                        
                        <span class="card-title">3d gaussian splating</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-03-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" class="post-category">
                                    ä¸‰ç»´é‡å»º
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/3dgs/">
                        <span class="chip bg-color">3dgs</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/03/05/stl-chang-yong-zhi-shi-dian/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/19.jpg" class="responsive-img" alt="STLå¸¸ç”¨çŸ¥è¯†ç‚¹">
                        
                        <span class="card-title">STLå¸¸ç”¨çŸ¥è¯†ç‚¹</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-03-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            å°ä¹ä¹
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/C-%E8%AF%AD%E8%A8%80/">
                        <span class="chip bg-color">C++è¯­è¨€</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- æ˜¯å¦åŠ è½½ä½¿ç”¨è‡ªå¸¦çš„ prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    <div class="card" data-aos="fade-up">
    <div id="utteranc-container" class="card-content">
        <script src="https://utteranc.es/client.js"
        repo="ZhenWusi/utterances"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
    </div>
</div>

    
    

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="3078651033"
                   fixed='true'
                   autoplay='true'
                   theme='#42b983'
                   loop='one'
                   order='random'
                   preload='none'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/about" target="_blank">å°ä¹ä¹</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://zhenwusi.github.io/" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1097407138@qq.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1097407138" class="tooltipped" target="_blank" data-tooltip="QQè”ç³»æˆ‘: 1097407138" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>





    <a href="https://www.zhihu.com/people/guo-guo-13-75-10" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/guo-guo-13-75-10" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
     
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/snow.js"><\/script>');
            }
        </script>
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
     
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/star.js"><\/script>');
            }
        </script>
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
